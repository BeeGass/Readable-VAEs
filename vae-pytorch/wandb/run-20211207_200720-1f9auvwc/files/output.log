
beginning experiment
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/train-images-idx3-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/train-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/train-labels-idx1-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/t10k-images-idx3-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/t10k-labels-idx1-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
9913344it [00:00, 19285939.16it/s]
29696it [00:00, 47703581.61it/s]
1649664it [00:00, 9404564.89it/s]
5120it [00:00, 15584061.31it/s]
beginning run
Epoch: 1
Train Loss: 0.22612141072750092
Test Loss: 0.2255374789237976
Epoch: 2
Train Loss: 0.2261219471693039
Test Loss: 0.22553345561027527
Epoch: 3
Train Loss: 0.22612746059894562
Test Loss: 0.2255323976278305
Epoch: 4
Train Loss: 0.22617611289024353
Test Loss: 0.22562165558338165
Traceback (most recent call last):
  File "main.py", line 66, in <module>
    config_run()
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/main.py", line 48, in decorated_main
    _run_hydra(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 377, in _run_hydra
    run_and_report(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 378, in <lambda>
    lambda: hydra.run(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 98, in run
    ret = run_job(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "main.py", line 36, in config_run
    run(model=model,
  File "main.py", line 51, in run
    train_loss, mod, dev = train(mod, optimizer, train_loader, dev, log_metrics, watch_loss)
  File "/home/beegass/Documents/Coding/Readable-VAEs/vae-pytorch/src/train_test/train.py", line 30, in train
    optimizer.step()
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/optim/adam.py", line 133, in step
    F.adam(params_with_grad,
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/optim/_functional.py", line 94, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt