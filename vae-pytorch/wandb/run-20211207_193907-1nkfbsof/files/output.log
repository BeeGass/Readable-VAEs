
beginning experiment
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/train-images-idx3-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/train-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/train-labels-idx1-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/t10k-images-idx3-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/t10k-labels-idx1-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
beginning run
9913344it [00:00, 20100123.80it/s]
29696it [00:00, 18914814.21it/s]
1649664it [00:00, 10831004.57it/s]
5120it [00:00, 9837304.85it/s]
Epoch: 0
Train Loss: 0.07336530089378357
Test Loss: 0.07472249120473862
Epoch: 1
Train Loss: 0.07331721484661102
Test Loss: 0.07485469430685043
Epoch: 2
Train Loss: 0.07310597598552704
Test Loss: 0.07482396066188812
Epoch: 3
Train Loss: 0.07327478379011154
Test Loss: 0.07468371838331223
Epoch: 4
Train Loss: 0.07303619384765625
Test Loss: 0.07480836659669876
Epoch: 5
Train Loss: 0.07306826859712601
Test Loss: 0.07462025433778763
Epoch: 6
Train Loss: 0.07273909449577332
Test Loss: 0.07493320852518082
Traceback (most recent call last):
  File "main.py", line 66, in <module>
    config_run()
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/main.py", line 48, in decorated_main
    _run_hydra(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 377, in _run_hydra
    run_and_report(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 378, in <lambda>
    lambda: hydra.run(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 98, in run
    ret = run_job(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "main.py", line 36, in config_run
    run(model=model,
  File "main.py", line 51, in run
    train_loss, mod, dev = train(mod, optimizer, train_loader, dev, log_metrics, watch_loss)
  File "/home/beegass/Documents/Coding/Readable-VAEs/vae-pytorch/src/train_test/train.py", line 30, in train
    optimizer.step()
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/optim/adam.py", line 133, in step
    F.adam(params_with_grad,
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/optim/_functional.py", line 87, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt