
beginning experiment
 26%|███▊           | 2543616/9912422 [00:00<00:00, 13477255.76it/s]
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
9913344it [00:00, 19793964.73it/s]
29696it [00:00, 50021707.46it/s]
1649664it [00:00, 9015457.47it/s]
5120it [00:00, 10748166.41it/s]
Extracting ./vae-pytorch/src/data/MNIST/raw/train-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/train-labels-idx1-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/t10k-images-idx3-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw/t10k-labels-idx1-ubyte.gz
Extracting ./vae-pytorch/src/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./vae-pytorch/src/data/MNIST/raw
beginning run
Epoch: 1
Train Loss: 0.07455623894929886
Test Loss: 0.07349172979593277
Epoch: 2
Train Loss: 0.07443699985742569
Test Loss: 0.073745496571064
Epoch: 3
Train Loss: 0.0743231400847435
Test Loss: 0.07369591295719147
Epoch: 4
Train Loss: 0.07447624951601028
Test Loss: 0.07368136197328568
Epoch: 5
Train Loss: 0.07440604269504547
Test Loss: 0.0733247846364975
Epoch: 6
Train Loss: 0.07476677745580673
Test Loss: 0.07365361601114273
Epoch: 7
Train Loss: 0.07445568591356277
Test Loss: 0.07386500388383865
Epoch: 8
Train Loss: 0.0746607705950737
Test Loss: 0.0736566111445427
Epoch: 9
Train Loss: 0.07480722665786743
Test Loss: 0.07379118353128433
Epoch     9: reducing learning rate of group 0 to 1.2000e-03.
Epoch: 10
Train Loss: 0.07110445201396942
Test Loss: 0.07384220510721207
Epoch: 11
Train Loss: 0.07101069390773773
Test Loss: 0.07393291592597961
Epoch: 12
Train Loss: 0.07100909948348999
Test Loss: 0.07392667233943939
Epoch: 13
Train Loss: 0.07102968543767929
Test Loss: 0.07389196753501892
Epoch: 14
Train Loss: 0.0710078775882721
Test Loss: 0.07393080741167068
Epoch: 15
Train Loss: 0.0710134282708168
Test Loss: 0.07393012940883636
Epoch: 16
Train Loss: 0.07105417549610138
Test Loss: 0.07387319207191467
Epoch: 17
Train Loss: 0.07103518396615982
Test Loss: 0.07388344407081604
Epoch    17: reducing learning rate of group 0 to 1.2000e-04.
Epoch: 18
Train Loss: 0.07046272605657578
Test Loss: 0.07393214851617813
Epoch: 19
Train Loss: 0.07045575231313705
Test Loss: 0.07392513751983643
Epoch: 20
Train Loss: 0.07045871764421463
Test Loss: 0.07392589747905731
Epoch: 21
Train Loss: 0.07045526057481766
Test Loss: 0.07392316311597824
Epoch: 22
Train Loss: 0.07045815885066986
Test Loss: 0.07392480969429016
Epoch: 23
Train Loss: 0.07045656442642212
Test Loss: 0.07392501085996628
Epoch: 24
Train Loss: 0.07046002149581909
Test Loss: 0.0739220678806305
Epoch: 25
Train Loss: 0.07045672088861465
Test Loss: 0.073921799659729
Epoch: 26
Train Loss: 0.07046114653348923
Test Loss: 0.07392226904630661
Epoch: 27
Train Loss: 0.07046099752187729
Test Loss: 0.07392199337482452
Epoch    27: reducing learning rate of group 0 to 1.2000e-05.
Epoch: 28
Train Loss: 0.07049154490232468
Test Loss: 0.07382805645465851
Epoch: 29
Train Loss: 0.07046784460544586
Test Loss: 0.07382600009441376
Epoch: 30
Train Loss: 0.0704578086733818
Test Loss: 0.07382450997829437
Epoch: 31
Train Loss: 0.07045356929302216
Test Loss: 0.07382402569055557
Epoch: 32
Train Loss: 0.07044488191604614
Test Loss: 0.07382484525442123
Epoch: 33
Train Loss: 0.07044611126184464
Test Loss: 0.07382319867610931
Epoch: 34
Train Loss: 0.0704367607831955
Test Loss: 0.07382302731275558
Epoch: 35
Train Loss: 0.07043907046318054
Test Loss: 0.07382509857416153
Epoch: 36
Train Loss: 0.07043294608592987
Test Loss: 0.07382462918758392
Epoch: 37
Train Loss: 0.0704326406121254
Test Loss: 0.07382258027791977
Epoch: 38
Train Loss: 0.07043564319610596
Test Loss: 0.07382356375455856
Epoch: 39
Train Loss: 0.07043282687664032
Test Loss: 0.07382317632436752
Epoch: 40
Train Loss: 0.0704326331615448
Test Loss: 0.07382331043481827
Epoch    40: reducing learning rate of group 0 to 1.2000e-06.
Epoch: 41
Train Loss: 0.07043460756540298
Test Loss: 0.0738079845905304
Epoch: 42
Train Loss: 0.07043282687664032
Test Loss: 0.0737990215420723
Epoch: 43
Train Loss: 0.07043503224849701
Test Loss: 0.07379432022571564
Epoch: 44
Train Loss: 0.07043658196926117
Test Loss: 0.07379289716482162
Epoch: 45
Train Loss: 0.07043547928333282
Test Loss: 0.07378888875246048
Epoch: 46
Train Loss: 0.0704369992017746
Test Loss: 0.07378745824098587
Epoch    46: reducing learning rate of group 0 to 1.2000e-07.
Epoch: 47
Train Loss: 0.07043643295764923
Test Loss: 0.07378559559583664
Epoch: 48
Train Loss: 0.07043586671352386
Test Loss: 0.07378501445055008
Epoch: 49
Train Loss: 0.07043659687042236
Test Loss: 0.07378508150577545
Epoch: 50
Train Loss: 0.07043620198965073
Test Loss: 0.07378467172384262
Epoch: 51
Train Loss: 0.07043807208538055
Test Loss: 0.0737869068980217
Epoch: 52
Train Loss: 0.07043634355068207
Test Loss: 0.07378524541854858
Epoch    52: reducing learning rate of group 0 to 1.2000e-08.
Epoch: 53
Train Loss: 0.07043663412332535
Test Loss: 0.07378492504358292
Epoch: 54
Train Loss: 0.07043716311454773
Test Loss: 0.07378614693880081
Epoch: 55
Train Loss: 0.0704362690448761
Test Loss: 0.07378578931093216
Epoch: 56
Train Loss: 0.07043628394603729
Test Loss: 0.07378508150577545
Epoch: 57
Train Loss: 0.07043567299842834
Test Loss: 0.07378506660461426
Epoch: 58
Train Loss: 0.07043632119894028
Test Loss: 0.073785699903965
Epoch    58: reducing learning rate of group 0 to 1.2000e-09.
Epoch: 59
Train Loss: 0.07043751329183578
Test Loss: 0.07378540188074112
Epoch: 60
Train Loss: 0.07043718546628952
Test Loss: 0.07378540933132172
Epoch: 61
Train Loss: 0.07043688744306564
Test Loss: 0.07378511130809784
Epoch: 62
Train Loss: 0.07043689489364624
Test Loss: 0.07378511875867844
Epoch: 63
Train Loss: 0.07043690979480743
Test Loss: 0.07378511875867844
Epoch: 64
Train Loss: 0.07043689489364624
Test Loss: 0.07378511875867844
Epoch: 65
Train Loss: 0.07043691724538803
Test Loss: 0.07378513365983963
Traceback (most recent call last):
  File "main.py", line 67, in <module>
    wandb.log({"lr": sched.get_lr()})
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/main.py", line 48, in decorated_main
    _run_hydra(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 377, in _run_hydra
    run_and_report(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 211, in run_and_report
    return func()
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/utils.py", line 378, in <lambda>
    lambda: hydra.run(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 98, in run
    ret = run_job(
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/hydra/core/utils.py", line 160, in run_job
    ret.return_value = task_function(task_cfg)
  File "main.py", line 36, in config_run
    mode='min',
  File "main.py", line 51, in run
    print("beginning run")
  File "/home/beegass/Documents/Coding/Readable-VAEs/vae-pytorch/src/train_test/train.py", line 17, in train
    x_hat, mu, log_var = model(img)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/beegass/Documents/Coding/Readable-VAEs/vae-pytorch/src/vae/vae.py", line 15, in forward
    mu, log_var = self._encoder(x)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/beegass/Documents/Coding/Readable-VAEs/vae-pytorch/src/encoder/vae_encoder.py", line 35, in forward
    latent_vector_z = self.encoder(x)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
KeyboardInterrupt